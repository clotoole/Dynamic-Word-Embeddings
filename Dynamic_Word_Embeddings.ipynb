{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.models\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input text-based data\n",
    "I process my data into csv format with each document labeled by title, year, and then content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/codyotoole/Desktop/Research_Data/GeneDrive_Data/gd_allyears_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of processing\n",
    "In my data a word of interest has multiple different uses, so I standardize them so the neural net interprets them as the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df)):\n",
    "    df['word'][i] = df['word'][i].replace('gene drive', 'gene_drive')\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    df['word'][i] = df['word'][i].replace('gene-drive', 'gene_drive')\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    df['word'][i] = df['word'][i].replace('Gene drive', 'gene_drive')\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    df['word'][i] = df['word'][i].replace('genetic drive', 'gene_drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Time Segments\n",
    "I am splitting the data into multiple data frames where each dataframe corresponds to a single year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for i, n in zip(range(1,17), range(2003, 2019)):\n",
    "    dfs[i] = df.loc[df['Date'] == n]\n",
    "    dfs[i].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log epochs \n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    \n",
    "    def __init__(self):\n",
    "         self.epoch = 0\n",
    "         \n",
    "    def on_epoch_begin(self, model):\n",
    "         print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "         print(\"Epoch #{} end\".format(self.epoch))\n",
    "         self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This makes a single w2v model for each time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = {}\n",
    "for i in range(1,17):\n",
    "    words = []\n",
    "    for n in range(0, len(dfs[i])):\n",
    "        w = dfs[i][\"word\"][n].split()\n",
    "        words.append(w)\n",
    "    m[i] = gensim.models.Word2Vec(min_count=1, window=5, size=100, \n",
    "                                   sample=1e-4, negative=10, workers=10)\n",
    "    m[i].build_vocab(words)  \n",
    "    epoch_logger = EpochLogger()\n",
    "    m[i].train(words, epochs =50, total_examples=m[i].corpus_count, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to align the gensim models to compare across time\n",
    "code pulled from https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        (With help from William. Thank you!)\n",
    "\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    \n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    base_embed.init_sims()\n",
    "    other_embed.init_sims()\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = in_base_embed.wv.syn0norm\n",
    "    other_vecs = in_other_embed.wv.syn0norm\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (other_embed.wv.syn0norm).dot(ortho)\n",
    "    return other_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = m.wv.syn0norm\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.wv.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop initialize the models to compare across time\n",
    "#keeps vocab so need to do reverse order to have largest vocab\n",
    "for i in reversed(range(2,17)):\n",
    "    print(i)\n",
    "    m[i-1] = smart_procrustes_align_gensim(m[i], m[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity \n",
    "def cos_sim(a,b):\n",
    "    result = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return result\n",
    "\n",
    "#Cosine Distance\n",
    "def cos_dis(a,b):\n",
    "    result = 1 - cos_sim(a,b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up x and y values for plot\n",
    "ycos = [cos_dis(m[1]['natural'],m[i+1]['natural']) for i in range(1,16)]\n",
    "ycos.insert(0,np.nan)\n",
    "x=[]\n",
    "for n in range(2003, 2019):\n",
    "    x.append(str(n))\n",
    "\n",
    "plot = pd.DataFrame({'x':x, 'y':ycos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Cosine Distance over time\n",
    "legend_properties = {'weight':'bold'}\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.plot (plot.x, plot.y, lw=2, linestyle='--', marker='o', \n",
    "          markersize=4, color='b', label='reverse')\n",
    "plt.xticks(range(0,len(x)), x)\n",
    "plt.legend(prop=legend_properties)\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('Cosine Distance', fontsize=12)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.title('Semantic Distance Since First Year', fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
